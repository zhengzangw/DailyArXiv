---
title: Latest 15 Papers - November 27, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Generation
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout](https://arxiv.org/abs/2511.20649v1)** | 2025-11-25 | <details><summary>Proje...</summary><p>Project Page: https://infinity-rope.github.io/</p></details> |
| **[Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization](https://arxiv.org/abs/2511.20647v1)** | 2025-11-25 | <details><summary>Proje...</summary><p>Project webpage: https://diverse-video.github.io/</p></details> |
| **[MotionV2V: Editing Motion in a Video](https://arxiv.org/abs/2511.20640v1)** | 2025-11-25 |  |
| **[iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation](https://arxiv.org/abs/2511.20635v1)** | 2025-11-25 |  |
| **[MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models](https://arxiv.org/abs/2511.20629v1)** | 2025-11-25 |  |
| **[ShapeGen: Towards High-Quality 3D Shape Synthesis](https://arxiv.org/abs/2511.20624v1)** | 2025-11-25 | <details><summary>Accep...</summary><p>Accepted to SIGGRAPH Asia 2025</p></details> |
| **[Multi-modal Generative AI: Multi-modal LLMs, Diffusions, and the Unification](https://arxiv.org/abs/2409.14993v3)** | 2025-11-25 | <details><summary>21 pa...</summary><p>21 pages, 10 figures, 3 tables</p></details> |
| **[E2E-GRec: An End-to-End Joint Training Framework for Graph Neural Networks and Recommender Systems](https://arxiv.org/abs/2511.20564v1)** | 2025-11-25 |  |
| **[A Reason-then-Describe Instruction Interpreter for Controllable Video Generation](https://arxiv.org/abs/2511.20563v1)** | 2025-11-25 | <details><summary>27 pa...</summary><p>27 pages, 13 figures, 13 tables, Project Page: https://sqwu.top/ReaDe/</p></details> |
| **[PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding](https://arxiv.org/abs/2511.20562v1)** | 2025-11-25 |  |
| **[AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations](https://arxiv.org/abs/2511.18617v2)** | 2025-11-25 | <details><summary>8 pag...</summary><p>8 pages, 6 figures. Code and datasets available at http://autofocus-il.github.io/</p></details> |
| **[ConceptGuard: Proactive Safety in Text-and-Image-to-Video Generation through Multimodal Risk Detection](https://arxiv.org/abs/2511.18780v2)** | 2025-11-25 |  |
| **[CGCE: Classifier-Guided Concept Erasure in Generative Models](https://arxiv.org/abs/2511.05865v2)** | 2025-11-25 | 26 pages, 17 figures |
| **[ExDDV: A New Dataset for Explainable Deepfake Detection in Video](https://arxiv.org/abs/2503.14421v2)** | 2025-11-25 | <details><summary>Accep...</summary><p>Accepted at WACV 2026</p></details> |
| **[STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow](https://arxiv.org/abs/2511.20462v1)** | 2025-11-25 | 21 pages |

## Diffusion
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[PixelDiT: Pixel Diffusion Transformers for Image Generation](https://arxiv.org/abs/2511.20645v1)** | 2025-11-25 |  |
| **[Image2Gcode: Image-to-G-code Generation for Additive Manufacturing Using Diffusion-Transformer Model](https://arxiv.org/abs/2511.20636v1)** | 2025-11-25 |  |
| **[Multi-modal Generative AI: Multi-modal LLMs, Diffusions, and the Unification](https://arxiv.org/abs/2409.14993v3)** | 2025-11-25 | <details><summary>21 pa...</summary><p>21 pages, 10 figures, 3 tables</p></details> |
| **[Latent Diffusion Inversion Requires Understanding the Latent Space](https://arxiv.org/abs/2511.20592v1)** | 2025-11-25 | <details><summary>14 pa...</summary><p>14 pages, 4 figures, 4 tables</p></details> |
| **[Anatomica: Localized Control over Geometric and Topological Properties for Anatomical Diffusion Models](https://arxiv.org/abs/2511.20587v1)** | 2025-11-25 | 8 pages, 10 figures |
| **[Localizing Knowledge in Diffusion Transformers](https://arxiv.org/abs/2505.18832v2)** | 2025-11-25 |  |
| **[Efficient and Fast Generative-Based Singing Voice Separation using a Latent Diffusion Model](https://arxiv.org/abs/2511.20470v1)** | 2025-11-25 | <details><summary>Accep...</summary><p>Accepted for oral presentation at IJCNN 2025</p></details> |
| **[Diffusion for Fusion: Designing Stellarators with Generative AI](https://arxiv.org/abs/2511.20445v1)** | 2025-11-25 |  |
| **[CardioComposer: Leveraging Differentiable Geometry for Compositional Control of Anatomical Diffusion Models](https://arxiv.org/abs/2509.08015v2)** | 2025-11-25 | 10 pages, 16 figures |
| **[FREE: Uncertainty-Aware Autoregression for Parallel Diffusion Transformers](https://arxiv.org/abs/2511.20390v1)** | 2025-11-25 |  |
| **[LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference](https://arxiv.org/abs/2510.11512v2)** | 2025-11-25 | 22 pages, 9 figures |
| **[Advancing Image Classification with Discrete Diffusion Classification Modeling](https://arxiv.org/abs/2511.20263v1)** | 2025-11-25 |  |
| **[Text-guided Controllable Diffusion for Realistic Camouflage Images Generation](https://arxiv.org/abs/2511.20218v1)** | 2025-11-25 | <details><summary>Accep...</summary><p>Accepted by AAAI 2026</p></details> |
| **[UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers](https://arxiv.org/abs/2511.20123v1)** | 2025-11-25 | <details><summary>Proje...</summary><p>Project page: https://thu-ml.github.io/UltraViCo.github.io/</p></details> |
| **[Towards A Tri-View Diffusion Framework for Recommendation](https://arxiv.org/abs/2511.20122v1)** | 2025-11-25 | <details><summary>13 pa...</summary><p>13 pages, 11 figures, accepted by KDD2026 (First Cycle)</p></details> |

